<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="RoBERTa," />










<meta name="description" content="Paperhttps:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1907.11692v1.pdf AuthorsYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov AbstractTraining i">
<meta property="og:type" content="article">
<meta property="og:title" content="論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach">
<meta property="og:url" content="http://pleomax0730.github.io/2020/04/03/roberta/index.html">
<meta property="og:site_name" content="論文閱讀筆記">
<meta property="og:description" content="Paperhttps:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1907.11692v1.pdf AuthorsYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov AbstractTraining i">
<meta property="og:image" content="https://i.imgur.com/uuoGwMk.jpg">
<meta property="og:image" content="https://i.imgur.com/qgxSo7o.jpg">
<meta property="og:image" content="https://i.imgur.com/1l8ee7G.jpg">
<meta property="og:image" content="https://i.imgur.com/LqSPxzo.jpg">
<meta property="og:image" content="https://i.imgur.com/My07obf.jpg">
<meta property="og:image" content="https://i.imgur.com/YBFYbgc.jpg">
<meta property="article:published_time" content="2020-04-03T07:37:36.000Z">
<meta property="article:modified_time" content="2020-04-03T07:37:36.000Z">
<meta property="article:author" content="Vincent Wu">
<meta property="article:tag" content="RoBERTa">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/uuoGwMk.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://pleomax0730.github.io/2020/04/03/roberta/"/>





  <title>論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach | 論文閱讀筆記</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">論文閱讀筆記</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://pleomax0730.github.io/2020/04/03/roberta/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.imgur.com/NJyO9ky.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="論文閱讀筆記">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach</h1>
        

        <div class="post-meta">
          
            <i class="fa fa-thumb-tack"></i>
            <font color="RED">置顶</font>
            <span class="post-meta-divider">|</span>
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-04-03T15:37:36+08:00">
                2020-04-03
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-04-03T15:37:36+08:00">
                2020-04-03
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computation-and-Language/" itemprop="url" rel="index">
                    <span itemprop="name">Computation and Language</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h2 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h2><p><a href="https://arxiv.org/pdf/1907.11692v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.11692v1.pdf</a></p>
<h2 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h2><p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, <strong>hyperparameter choices</strong> have significant impact on the final results.</p>
<p>We present a replication study of BERT pretraining (Devlin et al. , 2019) that carefully measures <strong>the impact of many key hyperparameters and training data size</strong>.</p>
<h3 id="BERT-問題點"><a href="#BERT-問題點" class="headerlink" title="BERT 問題點"></a>BERT 問題點</h3><p>We find that BERT was <strong>significantly undertrained</strong>, and can match or exceed the performance of every model published after it.</p>
<p>Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD.</p>
<p>These results highlight <strong>the importance of previously overlooked design choices</strong>, and raise questions about the source of recently reported improvements.<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="現行的預訓練模型問題點"><a href="#現行的預訓練模型問題點" class="headerlink" title="現行的預訓練模型問題點"></a>現行的預訓練模型問題點</h3><p>Self-training methods such as <strong>ELMo</strong> (Peters et al. , 2018), <strong>GPT</strong> (Radford et al. , 2018), <strong>BERT</strong> (Devlin et al. , 2019), <strong>XLM</strong> (Lample and Conneau , 2019), and <strong>XLNet</strong> (Yang et al. , 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most.</p>
<p>Training is <strong>computationally expensive</strong>, <strong>limiting the amount of tuning that can be done</strong>, and is often done with <strong>private training data of varying sizes</strong>, <strong>limiting our ability to measure the effects of the modeling advances</strong>.</p>
<h3 id="解決方法概述"><a href="#解決方法概述" class="headerlink" title="解決方法概述"></a>解決方法概述</h3><p>We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of <strong>hyperparmeter tuning and training set size</strong>.</p>
<p>We find that <strong>BERT was significantly undertrained</strong> and propose an improved recipe for training BERT models, which we call <strong>RoBERTa</strong>, that can match or exceed the performance of all of the post-BERT methods.</p>
<h3 id="RoBERTa-不同之處"><a href="#RoBERTa-不同之處" class="headerlink" title="RoBERTa 不同之處"></a>RoBERTa 不同之處</h3><p>Our modifications are simple, they include:</p>
<ol>
<li><strong>Training the model longer, with bigger batches, over more data;</strong></li>
<li><strong>Removing the next sentence prediction(NSP) objective;</strong></li>
<li><strong>Training on longer sequences;</strong></li>
<li><strong>Dynamically changing the masking pattern applied to the training data</strong>.</li>
</ol>
<p>We also collect a large new dataset <strong>(CC-NEWS) of comparable size to other privately used datasets</strong>, <strong>to better control for training set size effects.</strong></p>
<h3 id="本篇貢獻"><a href="#本篇貢獻" class="headerlink" title="本篇貢獻"></a>本篇貢獻</h3><ol>
<li><p>We <strong>present a set of important BERT design choices and training strategies and introduce alternatives</strong> that lead to better downstream task performance;</p>
</li>
<li><p>We use a novel dataset, CCNEWS, and confirm that <strong>using more data for pretraining further improves performance on downstream tasks</strong>;</p>
</li>
<li><p>Our training improvements show that <strong>masked language model pretraining, under the right design choices, is competitive with all other recently published methods</strong>.</p>
</li>
</ol>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section.</p>
<h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>BERT takes as input a concatenation of two segments (sequences of tokens), $x_{1}, \ldots, x_{N}$ and $y_{1}, \ldots, y_{M} .$ Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: $[C L S], x_{1}, \ldots, x_{N},[S E P], y_{1}, \ldots, y_{M},[E O S]$</p>
<p>$M$ and $N$ are constrained such that $M+N&lt;T$ where <strong>$T$ is a parameter that controls the maximum sequence length during training</strong>.</p>
<p>The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data.</p>
<h3 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h3><p>During pretraining, BERT uses two objectives: <strong>masked language modeling</strong> and <strong>next sentence prediction</strong>.</p>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: $\beta_{1}=0.9$ $\beta_{2}=0.999, \epsilon=1 \mathrm{e}-6$ and $L_{2}$ weight decay of $0.01 .$</p>
<p>The learning rate is <strong>warmed up over the first 10,000 steps to a peak value of 1e-4</strong>, and <strong>then linearly decayed</strong>.</p>
<p>BERT trains with a <strong>dropout of 0.1</strong> on all layers and attention weights, and a <strong>GELU activation function</strong> (Hendrycks and Gimpel, 2016).</p>
<p>Models are pretrained for <strong>$S=1,000,000$ updates</strong>, with <strong>minibatches containing $B=256$ sequences</strong> of <strong>maximum length $T=512$ tokens</strong>.</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>BERT is trained on a combination of BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA, which <strong>totals 16GB of uncompressed text</strong>. (Yang et al. (2019) use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data.)</p>
<h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><p>In this section, we describe the experimental setup for our replication study of BERT.</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>We reimplement BERT in FAIRSEQ (Ott et al., 2019).</p>
<p>We primarily follow the original BERT optimization hyperparameters, given in Section 2 <strong>except for the peak learning rate and number of warmup steps</strong>, which are <strong>tuned separately for each setting</strong>.</p>
<p>We additionally found training to be <strong>very sensitive to the Adam epsilon term</strong>, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found <strong>setting $\beta_{2}=0.98$ to improve stability when training with large batch sizes</strong>.</p>
<p>We <strong>pretrain with sequences of at most $T=512$ tokens</strong>.</p>
<p>Unlike Devlin et al. (2019), <strong>we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first $90 \%$ of updates</strong>. We <strong>train only with full-length sequences</strong>.</p>
<p>We <strong>train with mixed precision floating point arithmetic</strong> on DGX-1 machines, each with $8 \times$ $32 \mathrm{GB}$ Nvidia $\mathrm{V} 100$ GPUs interconnected by Infiniband (Micikevicius et al., 2018 ).</p>
<h3 id="Extra-Data"><a href="#Extra-Data" class="headerlink" title="Extra Data"></a>Extra Data</h3><p>We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora:</p>
<ol>
<li><p>BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA. This is the original data used to train BERT. <strong>(16GB)</strong>.</p>
</li>
<li><p>CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. <strong>(76GB after filtering)</strong>.</p>
</li>
<li><p>OPENWEBTEXT (Gokaslan and Cohen, 2019), an open-source recreation of the WebText corpus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. <strong>(38GB)</strong>.</p>
</li>
<li><p>STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. <strong>(31GB)</strong>.</p>
</li>
</ol>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>For SQuAD V1.1 we adopt the same span prediction method as BERT (Devlin et al., 2019).</p>
<p>For SQuAD V2.0, <strong>we add an additional binary classifier to predict whether the question is answerable</strong>, which we <strong>train jointly by summing the classification and span loss terms</strong>. <strong>During evaluation, we only predict span indices on pairs that are classified as answerable</strong>.</p>
<h2 id="Training-Procedure-Analysis"><a href="#Training-Procedure-Analysis" class="headerlink" title="Training Procedure Analysis"></a>Training Procedure Analysis</h2><p>This section explores and quantifies which choices are important for successfully pretraining BERT models. We keep the model architecture fixed. Specifically, we begin by training BERT models with the same configuration as $\mathrm{BERT}_{\mathrm{BASE}}(L=$ $12, H=768, A=12, 110 \mathrm{M}\text { params})$</p>
<h3 id="Static-vs-Dynamic-Masking"><a href="#Static-vs-Dynamic-Masking" class="headerlink" title="Static vs. Dynamic Masking"></a>Static vs. Dynamic Masking</h3><p>BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single <strong>static</strong> mask.</p>
<p><strong>To avoid using the same mask</strong> for each training instance in every epoch, <strong>training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training</strong>. Thus, each training sequence was seen with the same mask four times during training.</p>
<p>We compare this strategy with <strong>dynamic masking</strong> where <strong>we generate the masking pattern every time we feed a sequence to the model</strong>. <strong>This becomes crucial when pretraining for more steps or with larger datasets</strong>.</p>
<p><strong>Results Table 1</strong> compares the published BERT $_{\text {BASE }}$ results from Devlin et al. (2019) to our reimplementation <strong>with either static or dynamic masking</strong>.</p>
<p>We find that our <strong>reimplementation with static masking performs similar to the original BERT model</strong>, and dynamic masking is <strong>comparable or slightly better than static masking</strong>.</p>
<p>Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments.</p>
<p><img src="https://i.imgur.com/uuoGwMk.jpg" alt="Table1"></p>
<h3 id="Model-Input-Format-and-Next-Sentence-Prediction"><a href="#Model-Input-Format-and-Next-Sentence-Prediction" class="headerlink" title="Model Input Format and Next Sentence Prediction"></a>Model Input Format and Next Sentence Prediction</h3><p>In the original BERT pretraining procedure, the model observes two concatenated document segments, which are <strong>either sampled contiguously from the same document (with $p=0.5$ ) or from distinct documents</strong>.</p>
<p>In addition to the masked language modeling objective, <strong>the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss</strong>.</p>
<p><strong>The NSP loss was hypothesized to be an important factor in training the original BERT model</strong>. Devlin et al. ( 2019 ) observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1 However, <strong>some recent work has questioned the necessity of the NSP loss</strong> (Lample and Conneau, $2019 ; \text { Yang et al., } 2019 ; \text { Joshi et al., } 2019)$</p>
<p>To better understand this discrepancy, we compare several alternative training formats:</p>
<ul>
<li><p>SEGMENT-PAIR+NSP: This follows the original input format used in BERT (Devlin et al., 2019 ), with the NSP loss. <strong>Each input has a pair of segments, which can each contain multiple natural sentences</strong>, <strong>but the total combined length must be less than 512 tokens</strong>.</p>
</li>
<li><p>SENTENCE-PAIR+NSP: <strong>Each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents</strong>. since <strong>these inputs are significantly shorter than 512 tokens, we increase the batch size</strong> so that <strong>the total number of tokens remains similar to SEGMENT-PAIR+NSP</strong>. We <strong>retain the NSP loss</strong>.</p>
</li>
<li><p>FULL-SENTENCES: <strong>Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens</strong>. Inputs may cross document boundaries. <strong>When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents</strong>. We <strong>remove the NSP loss</strong>.</p>
</li>
<li><p>DOC-SENTENCES: <strong>Inputs are constructed similarly to FULL-SENTENCES, except that they may not cross document boundaries</strong>. Inputs sampled near the end of a document <strong>may be shorter than 512 tokens</strong>, so we <strong>dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULLSENTENCES</strong>. We <strong>remove the NSP loss</strong>.</p>
</li>
</ul>
<p><img src="https://i.imgur.com/qgxSo7o.jpg" alt="Table 2"></p>
<p><strong>Results Table 2</strong> shows results for the four different settings.</p>
<p>We first compare <strong>the original SEGMENT-PAIR input format</strong> from Devlin et al. (2019) to <strong>the SENTENCE-PAIR format</strong>; <strong>both formats retain the NSP loss</strong>, but the <strong>latter uses single sentences</strong>.</p>
<p>We find that <strong>using individual sentences hurts performance on downstream tasks</strong>, which we <strong>hypothesize is because the model is not able to learn long-range dependencies</strong>.</p>
<p>We next compare <strong>training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES)</strong>. We find that <strong>this setting outperforms the originally published $\mathrm{BERT}_{\mathrm{BASE}}$ results and that removing the NSP loss matches or slightly improves downstream task performance</strong>, in contrast to Devlin et al. (2019).</p>
<p>It is <strong>possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format</strong>.</p>
<p>Finally we find that <strong>restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES)</strong>.</p>
<p>However, because the DOC-SENTENCES format results in variable batch sizes, we use FULLSENTENCES in the remainder of our experiments for easier comparison with related work.</p>
<h3 id="Training-with-large-batches"><a href="#Training-with-large-batches" class="headerlink" title="Training with large batches"></a>Training with large batches</h3><p>Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately (Ott et al., 2018). Recent work has shown that <strong>BERT is also amenable to large batch training (You et al., 2019)</strong>.</p>
<p>Devlin et al. (2019) <strong>originally trained BERT $_{\text {BASE }}$ for $1 \mathrm{M}$ steps with a batch size of 256 sequences</strong>. This is <strong>equivalent in computational cost, via gradient accumulation</strong>, to training <strong>for $125 \mathrm{K}$ steps with a batch size of $2 \mathrm{K}$ sequences</strong>, or <strong>for $31 \mathrm{K}$ steps with a batch size of $8 \mathrm{K}$</strong>.</p>
<p><img src="https://i.imgur.com/1l8ee7G.jpg" alt="Table 3"></p>
<p>As we increase the batch size, we observe that <strong>training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy</strong>. Large batches are also <strong>easier to parallelize via distributed data parallel training</strong>, and <strong>in later experiments we train with batches of 8K sequences</strong>.</p>
<p>Notably <strong>You et al. (2019) train BERT with</strong> even larger batche sizes, up to <strong>32K sequences</strong>. We leave further exploration of the limits of large batch training to future work.</p>
<h3 id="Text-Encoding"><a href="#Text-Encoding" class="headerlink" title="Text Encoding"></a>Text Encoding</h3><p>The original BERT implementation (Devlin et al., 2019 ) uses a <strong>character-level BPE vocabulary of size $30 \mathrm{K},$</strong> <strong>which is learned after preprocessing the input with heuristic tokenization rules</strong>. Following Radford et al. (2019), <strong>we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input</strong>. This <strong>adds approximately 15M and 20M additional parameters</strong> for BERT $_{\text {BASE }}$ and BERT $_{\text {LARGE }}$, respectively.</p>
<p>Early experiments revealed only slight differences between these encodings, <strong>with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks</strong>. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments.</p>
<h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><p>In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration <strong>RoBERTa</strong> for <strong>R</strong>obustly <strong>o</strong>ptimized <strong>BERT a</strong>pproach.</p>
<p>Specifically, RoBERTa is trained with <strong>dynamic masking</strong> (Section 4.1), <strong>FULL-SENTENCES without NSP loss</strong> (Section 4.2), <strong>large mini-batches</strong> (Section 4.3) and a <strong>larger byte-level BPE</strong> (Section 4.4).</p>
<p>Additionally, we investigate two other important factors that have been under-emphasized in previous work:</p>
<ol>
<li>the data used for pretraining</li>
<li>the number of training passes through the data</li>
</ol>
<p>For example, the recently proposed <strong>XLNet</strong> architecture (Yang et al., 2019) is <strong>pretrained using nearly 10 times more data than the original BERT</strong> (Devlin et al., 2019). It is <strong>also trained with a batch size eight times larger</strong> for <strong>half as many optimization steps</strong>, thus seeing four times as many sequences in pretraining compared to BERT.</p>
<p>To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we <strong>begin by training RoBERTa following the BERT_LARGE architecture $(L=24$ $H=1024, A=16,355 \mathrm{M}$ parameters pretrain for $100 \mathrm{K}$ steps over a comparable BOOK CORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019)</strong>. We pretrain our model using <strong>1024 V100 GPUs for approximately one day</strong>.</p>
<p><img src="https://i.imgur.com/LqSPxzo.jpg" alt="Table 4"></p>
<p>Results We present our results in Table 4. When <strong>controlling for training data</strong>, we observe that RoBERTa provides a large improvement over the originally reported BERT $_{\text {LARGE }}$ results, <strong>reaffirming the importance of the design choices we explored in Section 4</strong>.</p>
<p>Next, <strong>we combine this data with the three additional datasets described in Section 3.2</strong>. We train RoBERTa over the combined data <strong>with the same number of training steps as before (100K)</strong>. In total, we pretrain <strong>over 160GB of text</strong>. We observe <strong>further improvements in performance across all downstream tasks</strong>, validating the <strong>importance of data size and diversity in pretraining</strong>.</p>
<p>Finally, we pretrain RoBERTa for significantly longer, <strong>increasing the number of pretraining steps from 100K to 300K, and then further to 500K</strong>. We again observe significant gains in downstream task performance, and the <strong>300K and 500K step models outperform XLNet-LARGE across most tasks</strong>. We note that <strong>even our longest-trained model does not appear to overfit our data and would likely benefit from additional training</strong>.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We carefully evaluate a number of design decisions when pretraining BERT models.</p>
<p>We find that performance can be substantially improved</p>
<ul>
<li><strong>Training the model longer with bigger batches over more data</strong></li>
<li><strong>Removing the next sentence prediction objective</strong></li>
<li><strong>Training on longer sequences</strong></li>
<li><strong>Dynamically changing the masking pattern applied to the training data</strong></li>
</ul>
<p>Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task finetuning for GLUE or additional data for SQuAD. These results illustrate the importance of these previously overlooked design decisions and suggest that BERT’s pretraining objective remains competitive with recently proposed alternatives.</p>
<p><strong>We additionally use a novel dataset, CC-NEWS</strong>, and release our models and code for pretraining and finetuning at: <a href="https://github.com/pytorch/fairseq." target="_blank" rel="noopener">https://github.com/pytorch/fairseq.</a></p>
<h2 id="Pretraining-Hyperparameters"><a href="#Pretraining-Hyperparameters" class="headerlink" title="Pretraining Hyperparameters"></a>Pretraining Hyperparameters</h2><p><img src="https://i.imgur.com/My07obf.jpg" alt="Table 9"></p>
<h2 id="Finetuning-Hyperparameters"><a href="#Finetuning-Hyperparameters" class="headerlink" title="Finetuning Hyperparameters"></a>Finetuning Hyperparameters</h2><p><img src="https://i.imgur.com/YBFYbgc.jpg" alt="Table 10"></p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Vincent Wu
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://pleomax0730.github.io/2020/04/03/roberta/" title="論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach">http://pleomax0730.github.io/2020/04/03/roberta/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/RoBERTa/" rel="tag"># RoBERTa</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/24/gpt-2/" rel="next" title="論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners">
                <i class="fa fa-chevron-left"></i> 論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.imgur.com/NJyO9ky.jpg"
                alt="Vincent Wu" />
            
              <p class="site-author-name" itemprop="name">Vincent Wu</p>
              <p class="site-description motion-element" itemprop="description">While not succeed: try()</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper"><span class="nav-number">1.</span> <span class="nav-text">Paper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors"><span class="nav-number">2.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">3.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT-問題點"><span class="nav-number">3.1.</span> <span class="nav-text">BERT 問題點</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">4.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#現行的預訓練模型問題點"><span class="nav-number">4.1.</span> <span class="nav-text">現行的預訓練模型問題點</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解決方法概述"><span class="nav-number">4.2.</span> <span class="nav-text">解決方法概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoBERTa-不同之處"><span class="nav-number">4.3.</span> <span class="nav-text">RoBERTa 不同之處</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本篇貢獻"><span class="nav-number">4.4.</span> <span class="nav-text">本篇貢獻</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">5.</span> <span class="nav-text">Background</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Setup"><span class="nav-number">5.1.</span> <span class="nav-text">Setup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Objectives"><span class="nav-number">5.2.</span> <span class="nav-text">Training Objectives</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization"><span class="nav-number">5.3.</span> <span class="nav-text">Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data"><span class="nav-number">5.4.</span> <span class="nav-text">Data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experimental-Setup"><span class="nav-number">6.</span> <span class="nav-text">Experimental Setup</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementation"><span class="nav-number">6.1.</span> <span class="nav-text">Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Extra-Data"><span class="nav-number">6.2.</span> <span class="nav-text">Extra Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation"><span class="nav-number">6.3.</span> <span class="nav-text">Evaluation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Procedure-Analysis"><span class="nav-number">7.</span> <span class="nav-text">Training Procedure Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Static-vs-Dynamic-Masking"><span class="nav-number">7.1.</span> <span class="nav-text">Static vs. Dynamic Masking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Input-Format-and-Next-Sentence-Prediction"><span class="nav-number">7.2.</span> <span class="nav-text">Model Input Format and Next Sentence Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-with-large-batches"><span class="nav-number">7.3.</span> <span class="nav-text">Training with large batches</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-Encoding"><span class="nav-number">7.4.</span> <span class="nav-text">Text Encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RoBERTa"><span class="nav-number">8.</span> <span class="nav-text">RoBERTa</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">9.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pretraining-Hyperparameters"><span class="nav-number">10.</span> <span class="nav-text">Pretraining Hyperparameters</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Finetuning-Hyperparameters"><span class="nav-number">11.</span> <span class="nav-text">Finetuning Hyperparameters</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Wu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://pleomax0730.github.io/2020/04/03/roberta/';
          this.page.identifier = '2020/04/03/roberta/';
          this.page.title = '論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://vincentwu.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
