<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="XLMs," />










<meta name="description" content="Paperhttps:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1901.07291v1 AuthorsGuillaume Lample, Alexis Conneau Abstract主旨Recent studies have demonstrated the efficiency of generative pretraining for English natural language underst">
<meta property="og:type" content="article">
<meta property="og:title" content="論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining">
<meta property="og:url" content="http://pleomax0730.github.io/2020/03/09/xlms/index.html">
<meta property="og:site_name" content="論文閱讀筆記">
<meta property="og:description" content="Paperhttps:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1901.07291v1 AuthorsGuillaume Lample, Alexis Conneau Abstract主旨Recent studies have demonstrated the efficiency of generative pretraining for English natural language underst">
<meta property="og:image" content="https://imgur.com/uDVbqlL.jpg">
<meta property="article:published_time" content="2020-03-09T04:05:26.000Z">
<meta property="article:modified_time" content="2020-03-09T04:05:26.000Z">
<meta property="article:author" content="Vincent Wu">
<meta property="article:tag" content="XLMs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://imgur.com/uDVbqlL.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://pleomax0730.github.io/2020/03/09/xlms/"/>





  <title>論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining | 論文閱讀筆記</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">論文閱讀筆記</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://pleomax0730.github.io/2020/03/09/xlms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.imgur.com/NJyO9ky.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="論文閱讀筆記">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining</h1>
        

        <div class="post-meta">
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-09T12:05:26+08:00">
                2020-03-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-03-09T12:05:26+08:00">
                2020-03-09
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computation-and-Language/" itemprop="url" rel="index">
                    <span itemprop="name">Computation and Language</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h2 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h2><p><a href="https://arxiv.org/abs/1901.07291v1" target="_blank" rel="noopener">https://arxiv.org/abs/1901.07291v1</a></p>
<h2 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h2><p>Guillaume Lample, Alexis Conneau</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="主旨"><a href="#主旨" class="headerlink" title="主旨"></a>主旨</h3><p>Recent studies have demonstrated the efficiency of generative pretraining for <strong><em>English</em></strong> natural language understanding.</p>
<p>In this work, we extend this approach to <strong>multiple languages</strong> and show the effectiveness of <strong>cross-lingual pretraining</strong>.<br><a id="more"></a></p>
<h3 id="解決方法"><a href="#解決方法" class="headerlink" title="解決方法"></a>解決方法</h3><p>We propose <strong>two methods to learn cross-lingual language models (XLMs)</strong>:</p>
<ul>
<li><strong>Unsupervised</strong> that only relies on <strong>monolingual data</strong></li>
<li><strong>Supervised</strong> that leverages parallel data with a <strong>new cross-lingual language model objective</strong>.</li>
</ul>
<h3 id="SOTA"><a href="#SOTA" class="headerlink" title="SOTA"></a>SOTA</h3><ul>
<li><p><strong>XNLI</strong><br>Our approach pushes the state of the art by an absolute gain of 4.9% accuracy.</p>
</li>
<li><p><strong>Unsupervised machine translation</strong><br>We obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU.</p>
</li>
<li><p><strong>Supervised machine translation</strong><br>We obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU.</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="問題及現況描述"><a href="#問題及現況描述" class="headerlink" title="問題及現況描述"></a>問題及現況描述</h3><p>Although there has been a surge of interest in learning general-purpose sentence representations, research in that area has been essentially <strong>monolingual</strong>, and largely focused around English benchmarks (Conneau and Kiela, 2018; Wang et al., 2018).</p>
<p>Recent developments in learning and evaluating cross-lingual sentence representations in many languages (Conneau et al., 2018b) aim at <strong>mitigating the English-centric bias</strong> and suggest that it is possible to <strong>build universal cross-lingual encoders</strong> that can <strong>encode any sentence into a shared embedding space</strong>.</p>
<h3 id="貢獻"><a href="#貢獻" class="headerlink" title="貢獻"></a>貢獻</h3><p>In this work, we demonstrate the effectiveness of cross-lingual language model pretraining on multiple cross-lingual understanding (XLU) benchmarks.</p>
<p>Precisely, we make the following contributions:</p>
<ol>
<li><p>We introduce a new <strong>unsupervised</strong> method for <strong>learning cross-lingual representations</strong> using <strong>cross-lingual language modeling</strong> and investigate <strong>two monolingual pretraining objectives</strong>.</p>
</li>
<li><p>We introduce <strong>a new supervised learning objective</strong> that <strong>improves cross-lingual pretraining</strong> when parallel data is available.</p>
</li>
<li><p>We significantly outperform the previous state of the art on cross-lingual classification, unsupervised machine translation and supervised machine translation.</p>
</li>
<li><p>We show that cross-lingual language models can provide <strong>significant improvements on the perplexity of low-resource languages</strong>.</p>
</li>
<li><p>We will make our code and pretrained models publicly available.</p>
</li>
</ol>
<h2 id="XLMs-Cross-lingual-language-models"><a href="#XLMs-Cross-lingual-language-models" class="headerlink" title="XLMs: Cross-lingual language models"></a>XLMs: Cross-lingual language models</h2><p>In this section, we present the three language modeling objectives we consider throughout this work.</p>
<p><strong>Two of them only require monolingual data (unsupervised)</strong>, while <strong>the third one requires parallel sentences (supervised)</strong>.</p>
<p>We consider $N$ languages. Unless stated otherwise, we suppose that we have $N$ monolingual corpora $\left\{C_{i}\right\}_{i=1 \ldots N},$ and we denote by $n_{i}$ the number of sentences in $C_{i}$</p>
<h3 id="Shared-sub-word-vocabulary"><a href="#Shared-sub-word-vocabulary" class="headerlink" title="Shared sub-word vocabulary"></a>Shared sub-word vocabulary</h3><p>In all our experiments we process all languages with <strong>the same shared vocabulary</strong> created through <strong>Byte Pair Encoding (BPE) (Sennrich et al., 2015)</strong>.</p>
<p>As shown in Lample et al. (2018a), this greatly improves the alignment of embedding spaces across languages that share either the same alphabet or anchor tokens such as digits (Smith et al., 2017) or proper nouns.</p>
<p>We <strong>learn the BPE splits on the concatenation of sentences sampled randomly from the monolingual corpora</strong>. Sentences are sampled according to a <strong>multinomial distribution</strong> with probabilities $\left\{q_{i}\right\}_{i=1 \ldots N},$ where:</p>
<p>$$<br>q_{i}=\frac{p_{i}^{\alpha}}{\sum_{j=1}^{N} p_{j}^{\alpha}} \text { with } p_{i}=\frac{n_{i}}{\sum_{k=1}^{N} n_{k}}</p>
<p>$$ We consider $\alpha=0.5 $</p>
<p>Sampling with <strong>this distribution increases the number of tokens associated to low-resource languages</strong> and <strong>alleviates the bias towards high-resource languages</strong>.</p>
<p>In particular, this <strong>prevents words of low-resource languages from being split at the character level</strong>.</p>
<h3 id="Causal-Language-Modeling-CLM"><a href="#Causal-Language-Modeling-CLM" class="headerlink" title="Causal Language Modeling (CLM)"></a>Causal Language Modeling (CLM)</h3><p>Our <strong>causal language modeling (CLM) task</strong> consists of <strong>a Transformer language model trained to model the probability of a word given the previous words in a sentence</strong> $P\left(w_{t} | w_{1}, \ldots, w_{t-1}, \theta\right)$</p>
<p>While recurrent neural networks obtain state-of-the-art performance on language modeling benchmarks (Mikolov et al., 2010 ; Jozefowicz et al., 2016 ), Transformer models are also very competitive (Dai et al., 2019 ).</p>
<p>In the case of LSTM language models, backpropagation through time (Werbos, 1990 ) (BPTT) is performed by providing the LSTM with the last hidden state of the previous iteration. In the case of Transformers, previous hidden states can be passed to the current batch (Al-Rfou et al., 2018 ) to provide context to the first words in the batch. However, <strong>this technique does not scale to the cross-lingual setting</strong>, so we just leave the first words in each batch without context for simplicity.</p>
<h3 id="Masked-Language-Modeling-MLM"><a href="#Masked-Language-Modeling-MLM" class="headerlink" title="　Masked Language Modeling (MLM)"></a>　Masked Language Modeling (MLM)</h3><p>We also consider the <strong>masked language modeling (MLM) objective</strong> of Devlin et al. (2018), also known as the Cloze task (Taylor, 1953).</p>
<p>Following Devlin et al. (2018), we <strong>sample randomly 15% of the BPE tokens from the text streams</strong>, replace them by a <strong>[MASK] token 80% of the time</strong>, by a <strong>random token 10% of the time</strong>, and we keep them <strong>unchanged 10% of the time</strong>.</p>
<p>Differences between our approach and the MLM of Devlin et al. (2018) include the <strong>use of text streams of an arbitrary number of sentences (truncated at 256 tokens)</strong> instead of <strong>pairs of sentences</strong>.</p>
<p>To counter the imbalance between rare and frequent tokens (e.g. punctuations or stop words), we also subsample the frequent outputs using an approach similar to Mikolov et al. (2013b)：</p>
<ul>
<li>Tokens in a text stream are sampled according to a <strong>multinomial distribution</strong>, whose weights are <strong>proportional to the square root of their invert frequencies</strong>.</li>
</ul>
<p>Our MLM objective is illustrated in Figure 1.</p>
<p><img src="https://imgur.com/uDVbqlL.jpg" alt="Fig 1."></p>
<h3 id="Translation-Language-Modeling-TLM"><a href="#Translation-Language-Modeling-TLM" class="headerlink" title="Translation Language Modeling (TLM)"></a>Translation Language Modeling (TLM)</h3><p>Both the <strong>CLM and MLM objectives are unsupervised</strong> and <strong>only require monolingual data</strong>. However, <strong>these objectives cannot be used to leverage parallel data</strong> when it is available.</p>
<p>We introduce a new <strong>translation language modeling (TLM) objective for improving cross-lingual pretraining</strong>.</p>
<p>Our TLM objective is an extension of MLM, where instead of considering monolingual text streams, we <strong>concatenate parallel sentences</strong> as illustrated in Figure 1.</p>
<p>We <strong>randomly mask words in both the source and target sentences</strong>. To predict a word masked in an English sentence, the model can either attend to surrounding English words or to the French translation, encouraging the model to align the English and French representations.</p>
<p>In particular, the model can leverage the French context if the English one is not sufficient to infer the masked English words. <strong>To facilitate the alignment, we also reset the positions of target sentences</strong>.</p>
<h3 id="Cross-lingual-Language-Models"><a href="#Cross-lingual-Language-Models" class="headerlink" title="Cross-lingual Language Models"></a>Cross-lingual Language Models</h3><p>In this work, we consider cross-lingual language model pretraining with either CLM, MLM, or MLM used in combination with TLM.</p>
<p>For the <strong>CLM and MLM objectives</strong>, we train the model with batches of 64 streams of continuous sentences composed of <strong>256 tokens</strong>. At each iteration, a batch is composed of sentences coming from the same language, which is sampled from the distribution $\left\{q_{i}\right\}_{i=1 \ldots N}$ above, with $\alpha=0.7 .$</p>
<p>When TLM is used in combination with MLM, we alternate between these two objectives, and sample the language pairs with a similar approach.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We investigate two unsupervised training objectives that require only monolingual corpora: Causal Language Modeling (CLM) and Masked Language Modeling (MLM). We show that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.</p>
<p><strong>Without using a single parallel sentence</strong>, a cross-lingual language model <strong>fine-tuned on the XNLI cross-lingual classification benchmark already outperforms the previous supervised state of the art by 1.3% accuracy on average</strong>.</p>
<p>A key contribution of our work is</p>
<ul>
<li>The translation language modeling (TLM) objective which improves crosslingual language model pretraining by leveraging parallel data. TLM naturally extends the BERT MLM approach by using batches of parallel sentences instead of consecutive sentences.</li>
<li>We obtain a significant gain by using TLM in addition to MLM, and we show that this supervised approach beats the previous state of the art on XNLI by 4.9% accuracy on average.</li>
</ul>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Vincent Wu
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://pleomax0730.github.io/2020/03/09/xlms/" title="論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining">http://pleomax0730.github.io/2020/03/09/xlms/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/XLMs/" rel="tag"># XLMs</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/02/28/transformer-xl/" rel="next" title="論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context">
                <i class="fa fa-chevron-left"></i> 論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/03/24/gpt-2/" rel="prev" title="論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners">
                論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.imgur.com/NJyO9ky.jpg"
                alt="Vincent Wu" />
            
              <p class="site-author-name" itemprop="name">Vincent Wu</p>
              <p class="site-description motion-element" itemprop="description">While not succeed: try()</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper"><span class="nav-number">1.</span> <span class="nav-text">Paper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors"><span class="nav-number">2.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">3.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#主旨"><span class="nav-number">3.1.</span> <span class="nav-text">主旨</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解決方法"><span class="nav-number">3.2.</span> <span class="nav-text">解決方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SOTA"><span class="nav-number">3.3.</span> <span class="nav-text">SOTA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">4.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#問題及現況描述"><span class="nav-number">4.1.</span> <span class="nav-text">問題及現況描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#貢獻"><span class="nav-number">4.2.</span> <span class="nav-text">貢獻</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XLMs-Cross-lingual-language-models"><span class="nav-number">5.</span> <span class="nav-text">XLMs: Cross-lingual language models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Shared-sub-word-vocabulary"><span class="nav-number">5.1.</span> <span class="nav-text">Shared sub-word vocabulary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Causal-Language-Modeling-CLM"><span class="nav-number">5.2.</span> <span class="nav-text">Causal Language Modeling (CLM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Masked-Language-Modeling-MLM"><span class="nav-number">5.3.</span> <span class="nav-text">　Masked Language Modeling (MLM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Translation-Language-Modeling-TLM"><span class="nav-number">5.4.</span> <span class="nav-text">Translation Language Modeling (TLM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-lingual-Language-Models"><span class="nav-number">5.5.</span> <span class="nav-text">Cross-lingual Language Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">6.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Wu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://pleomax0730.github.io/2020/03/09/xlms/';
          this.page.identifier = '2020/03/09/xlms/';
          this.page.title = '論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://vincentwu.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
