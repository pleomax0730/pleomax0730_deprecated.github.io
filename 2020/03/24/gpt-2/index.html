<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="GPT-2," />










<meta name="description" content="Paperhttps:&#x2F;&#x2F;cdn.openai.com&#x2F;better-language-models&#x2F;language_models_are_unsupervised_multitask_learners.pdf AuthorsAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever Abstra">
<meta property="og:type" content="article">
<meta property="og:title" content="論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners">
<meta property="og:url" content="http://pleomax0730.github.io/2020/03/24/gpt-2/index.html">
<meta property="og:site_name" content="論文閱讀筆記">
<meta property="og:description" content="Paperhttps:&#x2F;&#x2F;cdn.openai.com&#x2F;better-language-models&#x2F;language_models_are_unsupervised_multitask_learners.pdf AuthorsAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever Abstra">
<meta property="og:image" content="https://i.imgur.com/WyEU2eu.jpg">
<meta property="og:image" content="https://i.imgur.com/OTGtlRg.jpg">
<meta property="article:published_time" content="2020-03-24T04:46:20.000Z">
<meta property="article:modified_time" content="2020-03-24T04:46:20.000Z">
<meta property="article:author" content="Vincent Wu">
<meta property="article:tag" content="GPT-2">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/WyEU2eu.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://pleomax0730.github.io/2020/03/24/gpt-2/"/>





  <title>論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners | 論文閱讀筆記</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">論文閱讀筆記</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://pleomax0730.github.io/2020/03/24/gpt-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.imgur.com/NJyO9ky.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="論文閱讀筆記">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners</h1>
        

        <div class="post-meta">
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-24T12:46:20+08:00">
                2020-03-24
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-03-24T12:46:20+08:00">
                2020-03-24
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computation-and-Language/" itemprop="url" rel="index">
                    <span itemprop="name">Computation and Language</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h2 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h2><p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<h2 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h2><p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="傳統NLP任務做法"><a href="#傳統NLP任務做法" class="headerlink" title="傳統NLP任務做法"></a>傳統NLP任務做法</h3><p>Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with <strong>supervised learning</strong> on taskspecific datasets.</p>
<h3 id="本篇內容簡述"><a href="#本篇內容簡述" class="headerlink" title="本篇內容簡述"></a>本篇內容簡述</h3><p>We demonstrate that language models begin to learn these tasks <strong>without any explicit supervision</strong> when trained on a new dataset of millions of webpages called WebText.</p>
<p><strong>The capacity of the language model is essential to the success of zero-shot task transfer</strong> and increasing it improves performance in a log-linear fashion across tasks.</p>
<p>Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but <strong>still underfits WebText</strong>.<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="當前問題簡述"><a href="#當前問題簡述" class="headerlink" title="當前問題簡述"></a>當前問題簡述</h3><p>Current machine learning systems are better characterized as narrow experts rather than competent generalists.</p>
<p>We would like to move towards more general systems which can perform many tasks – eventually <strong>without the need to manually create and label a training dataset for each one</strong>.</p>
<p>The prevalence of single task training on single domain datasets is a major contributor to the <strong>lack of generalization</strong> observed in current systems.</p>
<p>Progress towards robust systems with current architectures is likely to require training and measuring performance on a wide range of domains and tasks. Recently, several benchmarks have been proposed such as <strong>GLUE (Wang et al., 2018)</strong> and <strong>decaNLP (McCann et al., 2018)</strong> to begin studying this.</p>
<h3 id="本篇-Solution-簡述"><a href="#本篇-Solution-簡述" class="headerlink" title="本篇 Solution 簡述"></a>本篇 Solution 簡述</h3><p>We demonstrate language models can perform down-stream tasks in a <strong>zero-shot setting – without any parameter or architecture modification</strong>.</p>
<p>We demonstrate this approach shows potential by highlighting the ability of language models to perform a wide range of tasks in a zero-shot setting.</p>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>At <strong>the core of our approach is language modeling</strong>.</p>
<p>Language modeling is usually framed as <strong>unsupervised distribution estimation</strong> from a set of examples $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ each composed of variable length sequences of symbols $\left(s_{1}, s_{2}, \ldots, s_{n}\right) .$</p>
<p>Since language has a natural sequential ordering, it is common to <strong>factorize the joint probabilities over symbols as the product of conditional probabilities</strong> (Jelinek Q Mercer, 1980 ) (Bengio et al., 2003 ):</p>
<script type="math/tex; mode=display">
p(x)=\prod_{i=1}^{n} p\left(s_{n} | s_{1}, \ldots, s_{n-1}\right)</script><p>This approach allows for tractable sampling from and estimation of $p(x)$ as well as any conditionals of the form $p\left(s_{n-k}, \ldots, s_{n} | s_{1}, \ldots, s_{n-k-1}\right) .$</p>
<p>In recent years, there have been significant improvements in the expressiveness of models that can compute these conditional probabilities, such as self-attention architectures like the Transformer (Vaswani et al., 2017 ).</p>
<h3 id="語言模型容量的潛力"><a href="#語言模型容量的潛力" class="headerlink" title="語言模型容量的潛力"></a>語言模型容量的潛力</h3><p>The internet contains a vast amount of information that is passively available without the need for interactive communication.</p>
<p>Our speculation is that a <strong>language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement</strong>.</p>
<p>If a language model is able to do this it will be, in effect, performing unsupervised multitask learning. We test whether this is the case by analyzing the performance of language models in a zero-shot setting on a wide variety of tasks.</p>
<h3 id="Training-Dataset"><a href="#Training-Dataset" class="headerlink" title="Training Dataset"></a>Training Dataset</h3><p>All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a <strong>total of 40 GB of text</strong>.</p>
<p>We <strong>removed all Wikipedia documents from WebText</strong> since it is a common data source for other datasets and could complicate analysis due to <strong>overlapping</strong> training data with test evaluation tasks.</p>
<h3 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h3><p>Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical <strong>middle ground between character and word level language modeling</strong> which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences.</p>
<p>Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings. This would result in a base vocabulary of over 130,000 before any multi-symbol tokens are added. This is prohibitively large compared to the 32,000 to 64,000 token vocabularies often used with BPE.</p>
<p>In contrast, a byte-level version of BPE only requires a base vocabulary of size 256. However, directly applying BPE to the byte sequence results in suboptimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary.</p>
<p>We observed BPE including many versions of common words like dog since they occur in many variations such as <strong>dog. dog! dog?</strong>. This results in a sub-optimal allocation of limited vocabulary slots and model capacity.</p>
<p>To avoid this, we <strong>prevent BPE from merging across character categories for any byte sequence</strong>. We <strong>add an exception for spaces</strong> which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens.</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>We use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications.</p>
<p><strong>Layer normalization (Ba et al., 2016 ) was moved to the input of each sub-block</strong>, similar to a pre-activation residual network (He et al., 2016 ) and <strong>an additional layer normalization was added after the final self-attention block</strong>.</p>
<p>A modified initialization which accounts for the accumulation on the residual path with model depth is used. We <strong>scale the weights of residual layers</strong> at initialization by a factor of $1 / \sqrt{N}$ where $N$ is the number of residual layers. The <strong>vocabulary is expanded to $50,257 .$</strong> We also <strong>increase the context size from 512 to 1024 tokens</strong> and a larger batchsize of 512 is used.</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.imgur.com/WyEU2eu.jpg" alt="table2"></p>
<p>We trained and benchmarked four LMs with approximately log-uniformly spaced sizes.</p>
<p>The architectures are summarized in Table $2 .$ The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT (Devlin et al., 2018 ).</p>
<p>Our largest model, which we call GPT-2, has over an order of magnitude more parameters than GPT. <strong>The learning rate of each model was manually tuned for the best perplexity</strong> on a $5 \%$ held-out sample of WebText. <strong>All models still underfit WebText</strong> and held-out perplexity has as of yet improved given more training time.</p>
<h3 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h3><p>The <strong>Conversation Question Answering dataset (CoQA)</strong> Reddy et al. (2018) consists of documents from 7 different domains <strong>paired with natural language dialogues between a question asker and a question answerer about the document</strong>.</p>
<p><strong>CoQA tests reading comprehension capabilities</strong> and also <strong>the ability of models to answer questions</strong> that depend on conversation history (such as “Why?”).</p>
<p>Greedy decoding from GPT-2 when conditioned on a document, the history of the associated conversation, and a final token A: <strong>achieves 55 F1 on the development set</strong>.</p>
<p>This matches or exceeds the performance of 3 out of 4 baseline systems without using the 127,000+ manually collected question answer pairs those baselines were trained on.</p>
<p>The supervised SOTA, a <strong>BERT based system (Devlin et al.,2018), is nearing the 89 F1 performance</strong> of humans. While GPT-2’s performance is exciting for a system without any supervised training, some inspection of its answers and errors suggests GPT-2 often uses simple retrieval based heuristics such as <em>answer with a name from the document in response to a who question.</em></p>
<h3 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h3><p><img src="https://i.imgur.com/OTGtlRg.jpg" alt="table5"><br>A potential way to test what information is contained within a language model is to evaluate how often it generates the correct answer to factoid-style questions. Previous showcasing of this behavior in neural systems where all information is stored in parameters such as <em>A Neural Conversational Model</em> (Vinyals &amp; Le, 2015) reported qualitative results due to the lack of high-quality evaluation datasets.</p>
<p>The recently introduced <strong>Natural Questions dataset</strong> (Kwiatkowski et al., 2019) is a promising resource to test this more quantitatively. Similar to translation, <strong>the context of the language model</strong> is seeded with <strong>example question answer pairs</strong> which helps the model infer the short answer style of the dataset.</p>
<p><strong>GPT-2 answers 4.1% of questions correctly</strong> when evaluated by the <strong>exact match metric</strong> commonly used on reading comprehension datasets like SQUAD.</p>
<p>As a comparison point, the smallest model does not exceed the 1.0% accuracy of an incredibly simple baseline which returns the most common answer for each question type (who, what, where, etc…).</p>
<p>GPT-2 answers 5.3 times more questions correctly, suggesting that <strong>model capacity has been a major factor in the poor performance of neural systems</strong> on this kind of task as of yet.</p>
<p>The probability GPT-2 assigns to its generated answers is well calibrated and GPT-2 has an accuracy of 63.1% on the 1% of questions it is most confident in. <strong>The 30 most confident answers generated by GPT-2 on development set questions are shown in Table 5</strong>.</p>
<p>The performance of GPT-2 is still much, much, worse than the 30 to 50% range of open domain question answering systems which hybridize information retrieval with extractive document question answering (Alberti et al., 2019).</p>
<h2 id="Generalization-vs-Memorization"><a href="#Generalization-vs-Memorization" class="headerlink" title="Generalization vs Memorization"></a>Generalization vs Memorization</h2><p>For CoQA, <strong>about 15% of documents in the news domain are already in WebText</strong> and the model performs about 3 F1 better on these.</p>
<p>CoQA’s development set metric reports the average performance over 5 different domains and we measure a gain of about 0.5-1.0 F1 due to overlap across the various domains. However, <strong>no actual training questions or answers are in WebText</strong> since CoQA was released after the cutoff date for links in WebText.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><strong>When a large language model is trained on a sufficiently large and diverse dataset</strong> it is able to <strong>perform well across many domains and datasets</strong>.</p>
<p>GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language modeling datasets.</p>
<p>The diversity of tasks the model is able to perform in a zero-shot setting suggests that <strong>high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision</strong>.</p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Vincent Wu
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://pleomax0730.github.io/2020/03/24/gpt-2/" title="論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners">http://pleomax0730.github.io/2020/03/24/gpt-2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GPT-2/" rel="tag"># GPT-2</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/09/xlms/" rel="next" title="論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining">
                <i class="fa fa-chevron-left"></i> 論文閱讀筆記 XLMs：Cross-lingual Language Model Pretraining
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/04/03/roberta/" rel="prev" title="論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach">
                論文閱讀筆記 RoBERTa：A Robustly Optimized BERT Pretraining Approach <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.imgur.com/NJyO9ky.jpg"
                alt="Vincent Wu" />
            
              <p class="site-author-name" itemprop="name">Vincent Wu</p>
              <p class="site-description motion-element" itemprop="description">While not succeed: try()</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper"><span class="nav-number">1.</span> <span class="nav-text">Paper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors"><span class="nav-number">2.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">3.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#傳統NLP任務做法"><span class="nav-number">3.1.</span> <span class="nav-text">傳統NLP任務做法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本篇內容簡述"><span class="nav-number">3.2.</span> <span class="nav-text">本篇內容簡述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">4.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#當前問題簡述"><span class="nav-number">4.1.</span> <span class="nav-text">當前問題簡述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本篇-Solution-簡述"><span class="nav-number">4.2.</span> <span class="nav-text">本篇 Solution 簡述</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Approach"><span class="nav-number">5.</span> <span class="nav-text">Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#語言模型容量的潛力"><span class="nav-number">5.1.</span> <span class="nav-text">語言模型容量的潛力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Dataset"><span class="nav-number">5.2.</span> <span class="nav-text">Training Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Input-Representation"><span class="nav-number">5.3.</span> <span class="nav-text">Input Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-number">5.4.</span> <span class="nav-text">Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiments"><span class="nav-number">6.</span> <span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reading-Comprehension"><span class="nav-number">6.1.</span> <span class="nav-text">Reading Comprehension</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Question-Answering"><span class="nav-number">6.2.</span> <span class="nav-text">Question Answering</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalization-vs-Memorization"><span class="nav-number">7.</span> <span class="nav-text">Generalization vs Memorization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">8.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Wu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://pleomax0730.github.io/2020/03/24/gpt-2/';
          this.page.identifier = '2020/03/24/gpt-2/';
          this.page.title = '論文閱讀筆記 GPT-2：Language Models are Unsupervised Multitask Learners';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://vincentwu.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
