<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Transformer," />










<meta name="description" content="Abstract問題描述Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling.">
<meta property="og:type" content="article">
<meta property="og:title" content="論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context">
<meta property="og:url" content="http://pleomax0730.github.io/2020/02/28/transformer-xl/index.html">
<meta property="og:site_name" content="論文閱讀筆記">
<meta property="og:description" content="Abstract問題描述Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling.">
<meta property="og:image" content="https://i.imgur.com/LjNpFCL.jpg">
<meta property="og:image" content="https://imgur.com/Qb5gSgG.jpg">
<meta property="og:image" content="https://i.imgur.com/yEnwn54.jpg">
<meta property="article:published_time" content="2020-02-28T05:25:04.000Z">
<meta property="article:modified_time" content="2020-02-28T05:25:05.000Z">
<meta property="article:author" content="Vincent Wu">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/LjNpFCL.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://pleomax0730.github.io/2020/02/28/transformer-xl/"/>





  <title>論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context | 論文閱讀筆記</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">論文閱讀筆記</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://pleomax0730.github.io/2020/02/28/transformer-xl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Wu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.imgur.com/NJyO9ky.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="論文閱讀筆記">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context</h1>
        

        <div class="post-meta">
          
            <i class="fa fa-thumb-tack"></i>
            <font color="RED">置顶</font>
            <span class="post-meta-divider">|</span>
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-28T13:25:04+08:00">
                2020-02-28
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-02-28T13:25:05+08:00">
                2020-02-28
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computation-and-Language/" itemprop="url" rel="index">
                    <span itemprop="name">Computation and Language</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="問題描述"><a href="#問題描述" class="headerlink" title="問題描述"></a>問題描述</h3><p>Transformers have a potential of learning longer-term dependency, but are <strong>limited by a fixed-length context</strong> in the setting of language modeling.<br><a id="more"></a></p>
<h3 id="解決方法"><a href="#解決方法" class="headerlink" title="解決方法"></a>解決方法</h3><p>A neural architecture Transformer-XL that <strong>enables learning dependency beyond a fixed length</strong> <em>without disrupting temporal coherence</em>.</p>
<h3 id="模型架構機制"><a href="#模型架構機制" class="headerlink" title="模型架構機制"></a>模型架構機制</h3><p>It consists of a <strong>segment-level recurrence mechanism</strong> and a <strong>novel positional encoding scheme</strong>.</p>
<h3 id="機制效果"><a href="#機制效果" class="headerlink" title="機制效果"></a>機制效果</h3><p>The method not only <strong>enables capturing longer-term dependency</strong>, but also resolves <strong>the context fragmentation problem</strong>.</p>
<h3 id="TransformerXL-performance"><a href="#TransformerXL-performance" class="headerlink" title="TransformerXL performance"></a>TransformerXL performance</h3><p>TransformerXL learns dependency that is <strong>80% longer than RNNs</strong> and <strong>450% longer than vanilla Transformers</strong>, achieves better performance on both short and long sequences, and is up to <strong>1,800+times faster than vanilla Transformers during evaluation</strong>.</p>
<h3 id="SOTA"><a href="#SOTA" class="headerlink" title="SOTA"></a>SOTA</h3><div class="table-container">
<table>
<thead>
<tr>
<th>Method/Dataset</th>
<th>enwiki8</th>
<th>text8</th>
<th>One Billion Word</th>
<th>WT-103</th>
<th>PTB (w/o finetuning)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Previous Best</td>
<td>1.06</td>
<td>1.13</td>
<td>23.7</td>
<td>20.5</td>
<td>55.5</td>
</tr>
<tr>
<td>Transformer-XL</td>
<td><strong>0.99</strong></td>
<td><strong>1.08</strong></td>
<td><strong>21.8</strong></td>
<td><strong>18.3</strong></td>
<td><strong>54.5</strong></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="舊架構的一些問題"><a href="#舊架構的一些問題" class="headerlink" title="舊架構的一些問題"></a>舊架構的一些問題</h3><ul>
<li><p>It has been a challenge to equip neural networks with the capability to model long-term dependency in sequential data. Recurrent neural networks (RNNs), in particular Long Short-Term Memory (LSTM) networks, have been a standard solution to language modeling and obtained strong results on multiple benchmarks.</p>
</li>
<li><p>RNNs are <em>difficult to optimize</em> due to <strong>gradient vanishing and explosion (Hochreiter et al., 2001)</strong>, and the introduction of gating in LSTMs and the gradient clipping technique (Graves, 2013) might not be sufficient to fully address this issue.</p>
</li>
<li><p>Empirically, previous work has found that <strong>LSTM language models use 200 context words on average (Khandelwal et al., 2018)</strong>, indicating room for further improvement.</p>
</li>
</ul>
<h3 id="Context-Fragmentation-problem-上下文碎片化問題"><a href="#Context-Fragmentation-problem-上下文碎片化問題" class="headerlink" title="Context Fragmentation problem 上下文碎片化問題"></a>Context Fragmentation problem 上下文碎片化問題</h3><ul>
<li><p>As a consequence of the fixed context length, <strong>the model cannot capture any longer-term dependency beyond the predefined context length</strong>.</p>
</li>
<li><p>In addition, <strong>the fixed-length segments are created by selecting a consecutive chunk of symbols</strong> <em>without respecting the sentence or any other semantic boundary</em>. Hence, the model lacks necessary contextual information needed to well predict the first few symbols, leading to inefficient optimization and inferior performance. We refer to this problem as <strong><em>context fragmentation</em></strong>.</p>
</li>
</ul>
<h3 id="Context-Fragmentation-solution"><a href="#Context-Fragmentation-solution" class="headerlink" title="Context Fragmentation solution"></a>Context Fragmentation solution</h3><p>By proposing a new architecture called <strong>transformer-XL (meaning extra long)</strong>.</p>
<ul>
<li><p><strong>Segment-level recurrence mechanism</strong>：</p>
<ul>
<li><p>Introduce the notion of <strong>recurrence</strong> into our deep self-attention network. In particular, instead of computing the hidden states from scratch for each new segment, we <strong>reuse the hidden states obtained in previous segments</strong>.</p>
</li>
<li><p>The reused hidden states serve as <strong>memory</strong> for the current segment, which builds up a recurrent connection between the segments.</p>
</li>
</ul>
</li>
<li><p>Mechanism result：</p>
<ul>
<li><p>Modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections.</p>
</li>
<li><p>Passing information from the previous segment can also resolve the problem of context fragmentation.</p>
</li>
</ul>
</li>
</ul>
<h3 id="Relative-positional-encodings"><a href="#Relative-positional-encodings" class="headerlink" title="Relative positional encodings"></a>Relative positional encodings</h3><p>We show the necessity of <strong>using relative positional encodings</strong> rather than absolute ones, in order <strong>to enable state reuse</strong> without <em>causing temporal confusion</em>.</p>
<ul>
<li><p><strong>Positional encoding scheme</strong>：</p>
<ul>
<li>We introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.</li>
</ul>
</li>
</ul>
<h3 id="Power-of-Transformer-XL"><a href="#Power-of-Transformer-XL" class="headerlink" title="Power of Transformer-XL"></a>Power of Transformer-XL</h3><ul>
<li><p>Transformer-XL Obtained strong results on five datasets, varying from word-level to characterlevel language modeling.</p>
</li>
<li><p>Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.</p>
</li>
</ul>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Given a corpus of tokens $x = (x_1, . . . , x_T )$, the task of language modeling is to estimate the joint probability $P(x)$, which is often <strong>auto-regressively factorized as $P(x) = \prod_t P(x_t | x_{&lt;t})$</strong>. With the factorization, the problem reduces to estimating each conditional factor.</p>
<p>In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context $x_{&lt;t}$ into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.</p>
<h3 id="Vanilla-Transformer-Language-Models"><a href="#Vanilla-Transformer-Language-Models" class="headerlink" title="Vanilla Transformer Language Models"></a>Vanilla Transformer Language Models</h3><h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>In order to apply Transformer or self-attention to language modeling, the central problem is <strong>how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation</strong>.</p>
<h4 id="Simple-solution"><a href="#Simple-solution" class="headerlink" title="Simple solution"></a>Simple solution</h4><p>Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resource in practice.</p>
<h4 id="Feasible-but-crude-approximation"><a href="#Feasible-but-crude-approximation" class="headerlink" title="Feasible but crude approximation"></a>Feasible but crude approximation</h4><p>Split the entire corpus into shorter segments of manageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments.</p>
<p><img src="https://i.imgur.com/LjNpFCL.jpg" alt="Fig. 1"></p>
<p>This is the idea adopted by Al-Rfou et al. (2018). We call it the <em>vanilla model</em> and visualize it in <strong>Fig. 1a</strong>. Under this training paradigm, <strong>information never flows across segments in either the forward or backward pass</strong>.</p>
<h4 id="Two-critical-limitations-of-using-a-fixedlength-context"><a href="#Two-critical-limitations-of-using-a-fixedlength-context" class="headerlink" title="Two critical limitations of using a fixedlength context"></a>Two critical limitations of using a fixedlength context</h4><ol>
<li><p>The largest possible dependency length is <strong>upper bounded by the segment length</strong>, which is <em>a few hundred on character-level language modeling (Al-Rfou et al., 2018)</em>. Therefore, although the self-attention mechanism is less affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to fully exploit this optimization advantage.</p>
</li>
<li><p>Though it is possible to use <strong>padding</strong> to respect the sentence or other semantic boundaries, in practice it has been standard practice to <strong>simply chunk long text into fixed-length segments due to improved efficiency</strong> (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will <strong>lead to the context fragmentation problem</strong>.</p>
</li>
</ol>
<p><strong>During evaluation</strong>, at each step, the vanilla model also <strong>consumes a segment of the same length as in training</strong>, but <strong>only makes one prediction at the last position</strong>. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in <strong>Fig. 1b</strong>.</p>
<p>This procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, <strong>this evaluation procedure is extremely expensive</strong>.</p>
<p>We will show that our proposed architecture is able to substantially improve the evaluation speed.</p>
<h3 id="Segment-Level-Recurrence-with-State-Reuse"><a href="#Segment-Level-Recurrence-with-State-Reuse" class="headerlink" title="　Segment-Level Recurrence with State Reuse"></a>　Segment-Level Recurrence with State Reuse</h3><p><img src="https://imgur.com/Qb5gSgG.jpg" alt="Fig. 2"></p>
<h4 id="Solution-to-the-fixed-length-context"><a href="#Solution-to-the-fixed-length-context" class="headerlink" title="Solution to the fixed-length context"></a>Solution to the fixed-length context</h4><p>We propose to introduce a <strong>recurrence mechanism</strong> to the Transformer architecture.</p>
<h5 id="Training-phase"><a href="#Training-phase" class="headerlink" title="Training phase"></a>Training phase</h5><ul>
<li><p><strong>The hidden state sequence computed for the previous segment</strong> is <em>fixed and cached</em> <strong>to be reused</strong> as an extended context when the model processes the next new segment, as shown in <strong>Fig. 2a</strong>. Although <strong>the gradient still remains within a segment</strong>, this <strong>additional input</strong> allows the network to <strong>exploit information in the history</strong>, leading to an ability of modeling longer-term dependency and avoiding context fragmentation.</p>
</li>
<li><p>Formally, let the two consecutive segments of length $L$ be $\mathbf{s}_{\tau}=$ $\left[x_{\tau, 1}, \cdots, x_{\tau, L}\right]$ and $\mathbf{s}_{\tau+1}=\left[x_{\tau+1,1}, \cdots, x_{\tau+1, L}\right]$ respectively. Denoting <strong>the $n$ -th layer hidden state sequence produced for the $\tau$ -th segment $\mathbf{s}_{\tau}$ by $\mathbf{h}_{\tau}^{n} \in \mathbb{R}^{L \times d},$ where $d$ is the hidden dimension</strong>. Then, the $n$ -th layer hidden state for segment s $\tau+1$ is produced (schematically) as follows,</p>
</li>
</ul>
<blockquote>
<p>$\tilde{\mathbf{h}}_{\tau+1}^{n-1}=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right]$<br>$\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top}$<br>$\mathbf{h}_{\tau+1}^{n}=$ Transformer-Layer $\left(\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}\right)$</p>
</blockquote>
<ul>
<li><p>The function <strong>$\mathrm{SG}(\cdot)$ stands for stop-gradient</strong>, the notation <strong>$\left[\mathbf{h}_{u} \circ \mathbf{h}_{v}\right]$ indicates the concatenation of two hidden sequences along the length dimension</strong>, and $\mathbf{W}$. denotes model parameters.</p>
</li>
<li><p>Compared to the standard Transformer, the critical difference lies in that <strong>the key $\mathbf{k}_{\tau+1}^{n}$ and value $\mathbf{v}_{\tau+1}^{n}$ are conditioned on the extended context $\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ and hence $\mathbf{h}_{\tau}^{n-1}$ cached from the previous segment</strong>. We emphasize this particular design by the green paths in <strong>Fig. 2 a</strong>.</p>
</li>
</ul>
<h5 id="Evaluation-phase"><a href="#Evaluation-phase" class="headerlink" title="Evaluation phase"></a>Evaluation phase</h5><p>With this <strong>recurrence mechanism</strong> applied to every two consecutive segments of a corpus, it essentially creates a segment-level recurrence in the hidden states. As a result, the effective context being utilized can go way beyond just two segments.</p>
<p>However, notice that the recurrent dependency between $\mathbf{h}_{\tau+1}^{n}$ and $\mathbf{h}_{\tau}^{n-1}$ shifts one layer downwards per-segment, which differs from the same-layer recurrence in conventional RNN-LMs.</p>
<p>Consequently, the largest possible dependency length grows linearly w.r.t. the number of layers as well as the segment length, i.e., $O(N \times L),$ as visualized by the shaded area in <strong>Fig. $2 \mathrm{b} .$</strong> This is analogous to truncated BPTT (Mikolov et al., $2010),$ a technique developed for training RNN-LMs. However, different from truncated BPTT, <strong>our method caches a sequence of hidden states instead of the last one</strong>, and <strong>should be applied together with the relative positional encoding technique</strong> described in Section 3.3.</p>
<h3 id="Relative-Positional-Encodings"><a href="#Relative-Positional-Encodings" class="headerlink" title="Relative Positional Encodings"></a>Relative Positional Encodings</h3><h4 id="Problem-of-recurrence-mechanism"><a href="#Problem-of-recurrence-mechanism" class="headerlink" title="Problem of recurrence mechanism"></a>Problem of recurrence mechanism</h4><p>There is a crucial technical challenge we haven’t solved in order to reuse the hidden states. That is, <strong>how can we keep the positional information coherent when we reuse the states</strong>?</p>
<p>Recall that, in the standard Transformer, the information of sequence order is provided by a set of <strong>positional encodings</strong>, denoted as $\mathbf{U} \in \mathbb{R}^{L_{\max } \times d},$ where the $i$ -th row $\mathbf{U}_{i}$ corresponds to <strong>the $i$ -th <em>absolute</em> position within a segment</strong> and $L_{\text {max }}$ prescribes the maximum possible length to be modeled.</p>
<p>Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{h}_{\tau+1} &=f\left(\mathbf{h}_{\tau}, \mathbf{E}_{\mathbf{s}_{\tau+1}}+\mathbf{U}_{1: L}\right) \\
\mathbf{h}_{\tau} &=f\left(\mathbf{h}_{\tau-1}, \mathbf{E}_{\mathbf{s}_{\tau}}+\mathbf{U}_{1: L}\right)
\end{aligned}</script><p>where $\mathbf{E}_{\mathbf{s}_{\tau}} \in \mathbb{R}^{L \times d}$ is the word embedding sequence of $\mathbf{s}_{\tau},$ and $f$ represents a transformation function. Notice that, both $\mathbf{E}_{\mathbf{S}_{\tau}}$ and $\mathbf{E}_{\mathbf{S}_{\tau+1}}$ are associated with the same positional encoding $\mathbf{U}_{1: L}$</p>
<p>As a result, the model has no information to distinguish the positional difference between $x_{\tau, j}$ and $x_{\tau+1, j}$ for any $j=1, \ldots, L,$ resulting in a sheer performance loss.</p>
<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><p>In order to avoid this failure mode, the fundamental idea is to only encode the <strong><em>relative</em> positional information</strong> in the hidden states.</p>
<p>Conceptually, the <strong>positional encoding</strong> gives the model a temporal clue or “bias” about how information should be gathered, i.e., <strong>where to attend</strong>. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer.</p>
<p>More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.<br>For instance, when a query vector $q_{\tau, i}$ attends on the key vectors $\mathbf{k}_{\tau, \leq i},$ it does not need to know the absolute position of each key vector to identify the temporal order of the segment. Instead, it suffices to know the relative distance between each key vector $k_{\tau, j}$ and itself $q_{\tau, i}$, i.e. $i-j$.</p>
<p>Practically, one can create a set of relative positional encodings $\mathbf{R} \in \mathbb{R}^{L_{\max } \times d},$ where <strong>the $i$ -th row $\mathbf{R}_{i}$ indicates a relative distance of $i$ between two positions</strong>. <strong>By injecting the relative distance dynamically into the attention score</strong>, the query vector can easily distinguish the representations of $x_{\tau, j}$ and $x_{\tau+1, j}$ from their different distances, making the state reuse mechanism feasible. Meanwhile, we won’t lose any temporal information, as the absolute position can be recovered recursively from relative distances.</p>
<h4 id="Relative-Positional-Encodings-Formulation"><a href="#Relative-Positional-Encodings-Formulation" class="headerlink" title="Relative Positional Encodings Formulation"></a>Relative Positional Encodings Formulation</h4><p>Previously, the idea of relative positional encodings has been explored in the context of machine translation (Shaw et al., 2018) and music generation (Huang et al., 2018). Here, we offer a different derivation, arriving at a new form of relative positional encodings, which not only has a one-to-one correspondence to its absolute counterpart but also enjoys much better generalization empirically.</p>
<p>Firstly, in the standard Transformer (Vaswani et al., 2017 ), the attention score between query $q_{i}$ and key vector $k_{j}$ within the same segment can be decomposed as</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{A}_{i, j}^{\mathrm{abs}} &=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)} \\
&+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}
\end{aligned}</script><p>Following the idea of only relying on relative positional information, we propose to reparameterize the four terms as follows</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{A}_{i, j}^{\mathrm{rel}} &=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(b)} \\
&+\underbrace{u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}
\end{aligned}</script><ul>
<li>The first change we make is to replace all appearances of the <strong>absolute positional embedding $\mathbf{U}_{j}$</strong> for computing key vectors in term $(b)$ and (d) with its relative counterpart $\mathrm{R}_{i-j} .$ This essentially reflects the prior that only the relative distance matters for where to attend. Note that <strong>$\mathrm{R}$ is a sinusoid encoding matrix</strong> (Vaswani et al., 2017 ) without learnable parameters.</li>
<li>Secondly, we introduce <strong>a trainable parameter $u \in \mathbb{R}^{d}$</strong> to replace the query $\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top}$ in term (c). In this case, since the query vector is the same for all query positions, it suggests that <strong>the attentive bias towards different words should remain the same regardless of the query position</strong>. With a similar reasoning, a <strong>trainable parameter $v \in \mathbb{R}^{d}$</strong> is added to substitute $\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top}$ in term $(d)$.</li>
<li>Finally, we deliberately separate the two weight matrices $\mathbf{W}_{k, E}$ and $\mathbf{W}_{k, R}$ for producing the <strong>content-based key vectors</strong> and <strong>location-based key vectors</strong> respectively.</li>
</ul>
<p>Under the new parameterization, each term has an intuitive meaning: term ( $a$ ) represents <strong>content-based addressing</strong>, term ( $b$ ) captures a <strong>content-dependent positional bias</strong>, term ( $c$ ) governs a <strong>global content bias</strong>, and ( $d$ ) encodes a <strong>global positional bias</strong>.</p>
<p>In comparison, the formulation in Shaw et al. ( 2018) only has terms $(a)$ and $(b),$ dropping the two bias terms ( $c$ ) and ( $d$ ). Moreover, Shaw et al. ( 2018) merge the multiplication $\mathbf{W}_{k} \mathbf{R}$ into a single trainable matrix $\hat{\mathbf{R}},$ which abandons the inductive bias built into the original sinusoid positional encoding (Vaswani et al., 2017 ).</p>
<p>In contrast, our relative positional embedding $\mathbf{R}$ adapts the sinusoid formulation. As a benefit of the inductive bias, a model trained on a memory of some certain length can automatically generalize to a memory several times longer during evaluation.</p>
<h4 id="Summarize-the-computational-procedure"><a href="#Summarize-the-computational-procedure" class="headerlink" title="Summarize the computational procedure"></a>Summarize the computational procedure</h4><p>Equipping the <strong>recurrence mechanism</strong> with our proposed <strong>relative positional embedding</strong>, we finally arrive at the Transformer-XL architecture. For completeness, we summarize the computational procedure for a N-layer Transformer-XL with a single attention head here.</p>
<p>For $n=1, \ldots, N:$</p>
<p><img src="https://i.imgur.com/yEnwn54.jpg" alt="Formulation"></p>
<p>with <strong>$\mathbf{h}_{\tau}^{0}:=\mathbf{E}_{\mathbf{s}_{\tau}}$ defined as the word embedding sequence</strong>.</p>
<p>In addition, it is worth mentioning that a naive way to compute A requires computing $\mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j}$ for all pairs $(i, j),$ whose <strong>cost is quadratic w.r.t. the sequence length</strong>.</p>
<p>However, noticing that the value of $i-j$ only ranges from zero to the sequence length, we show a simple computation procedure in Appendix B, which <strong>reduces the cost to be linear w.r.t. the sequence length</strong>.</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Transformer-XL obtains strong perplexity results.</p>
<ul>
<li>Models longer-term dependency than RNNs and Transformer</li>
<li>Achieves substantial speedup during evaluation</li>
<li>Able to generate coherent text articles</li>
</ul>
<p>We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.</p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Vincent Wu
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://pleomax0730.github.io/2020/02/28/transformer-xl/" title="論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context">http://pleomax0730.github.io/2020/02/28/transformer-xl/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Transformer/" rel="tag"># Transformer</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/02/26/GPT/" rel="next" title="論文閱讀筆記 GPT：Improving Language Understanding by Generative Pre-Training">
                <i class="fa fa-chevron-left"></i> 論文閱讀筆記 GPT：Improving Language Understanding by Generative Pre-Training
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.imgur.com/NJyO9ky.jpg"
                alt="Vincent Wu" />
            
              <p class="site-author-name" itemprop="name">Vincent Wu</p>
              <p class="site-description motion-element" itemprop="description">While not succeed: try()</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#問題描述"><span class="nav-number">1.1.</span> <span class="nav-text">問題描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解決方法"><span class="nav-number">1.2.</span> <span class="nav-text">解決方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型架構機制"><span class="nav-number">1.3.</span> <span class="nav-text">模型架構機制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#機制效果"><span class="nav-number">1.4.</span> <span class="nav-text">機制效果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TransformerXL-performance"><span class="nav-number">1.5.</span> <span class="nav-text">TransformerXL performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SOTA"><span class="nav-number">1.6.</span> <span class="nav-text">SOTA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#舊架構的一些問題"><span class="nav-number">2.1.</span> <span class="nav-text">舊架構的一些問題</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Context-Fragmentation-problem-上下文碎片化問題"><span class="nav-number">2.2.</span> <span class="nav-text">Context Fragmentation problem 上下文碎片化問題</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Context-Fragmentation-solution"><span class="nav-number">2.3.</span> <span class="nav-text">Context Fragmentation solution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relative-positional-encodings"><span class="nav-number">2.4.</span> <span class="nav-text">Relative positional encodings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Power-of-Transformer-XL"><span class="nav-number">2.5.</span> <span class="nav-text">Power of Transformer-XL</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model"><span class="nav-number">3.</span> <span class="nav-text">Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer-Language-Models"><span class="nav-number">3.1.</span> <span class="nav-text">Vanilla Transformer Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem"><span class="nav-number">3.1.1.</span> <span class="nav-text">Problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple-solution"><span class="nav-number">3.1.2.</span> <span class="nav-text">Simple solution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feasible-but-crude-approximation"><span class="nav-number">3.1.3.</span> <span class="nav-text">Feasible but crude approximation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Two-critical-limitations-of-using-a-fixedlength-context"><span class="nav-number">3.1.4.</span> <span class="nav-text">Two critical limitations of using a fixedlength context</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Segment-Level-Recurrence-with-State-Reuse"><span class="nav-number">3.2.</span> <span class="nav-text">　Segment-Level Recurrence with State Reuse</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Solution-to-the-fixed-length-context"><span class="nav-number">3.2.1.</span> <span class="nav-text">Solution to the fixed-length context</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Training-phase"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">Training phase</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Evaluation-phase"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">Evaluation phase</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relative-Positional-Encodings"><span class="nav-number">3.3.</span> <span class="nav-text">Relative Positional Encodings</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem-of-recurrence-mechanism"><span class="nav-number">3.3.1.</span> <span class="nav-text">Problem of recurrence mechanism</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Solution"><span class="nav-number">3.3.2.</span> <span class="nav-text">Solution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Relative-Positional-Encodings-Formulation"><span class="nav-number">3.3.3.</span> <span class="nav-text">Relative Positional Encodings Formulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summarize-the-computational-procedure"><span class="nav-number">3.3.4.</span> <span class="nav-text">Summarize the computational procedure</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusions"><span class="nav-number">4.</span> <span class="nav-text">Conclusions</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Wu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://pleomax0730.github.io/2020/02/28/transformer-xl/';
          this.page.identifier = '2020/02/28/transformer-xl/';
          this.page.title = '論文閱讀筆記 Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://vincentwu.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
